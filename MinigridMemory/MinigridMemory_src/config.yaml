wandb_config:
    project_name: "v2-RATE-MinigridMemory"
    wwandb: True
    wcomet: False

data_config:
  gamma: 1.0
  normalize: 1 # 0 - not normalize, 1 - /255., 2 - standard scaling

training_config:
  learning_rate: 0.0003 # 1e-3
  lr_end_factor: 0.1 # *  old: 0.001
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  batch_size: 64 # 128
  warmup_steps: 10000  # 100
  final_tokens: 1000000
  grad_norm_clip: 1.0 # 1.0
  epochs: 250 #250
  ckpt_epoch: 2 # 25
  use_erl_stop: False
  # inference during training
  online_inference: True
  log_last_segment_loss_only: False
  use_cosine_decay: True
  # ! WARNING ! IMPORTANT INFO
  # * IF YOU WANT TO TRAIN DT WITH context_length=90 (or if you want to train RATE with
  # * 3 segments and context 30), set context_length=30 and sections=3
  context_length: 10 # 10  # if RATE/GRATE: L = L, if DT: L = sections * L
  sections: 3 # 3        # if RATE/GRATE: S = S, if DT: S = 1


model_config:
  mode: "minigrid_memory"
  STATE_DIM: 3
  ACTION_DIM: 4 # 2 if OHE else 4
  n_layer: 8 # 3 8
  n_head: 10 # 1 10
  n_head_ca: 2 # ! 2 | number of cross-attention heads
  d_model: 64 # 128
  d_head: 64 # 128 # divider of d_model
  d_inner: 64 # 128 # > d_model 
  dropout: 0.2 # 0.2
  dropatt: 0.05 # 0.05
  mem_len: 180
  ext_len: 0 # 0
  num_mem_tokens: 15 # 5 d_head * nmt = diff params
  mem_at_end: True
  mrv_act: 'relu'
  skip_dec_ffn: True

  padding_idx: -10 # None

online_inference_config:
  use_argmax: False # False
  episode_timeout: 100 # 2100
  desired_return_1: 0.96 # 0.995 # 56.5

tensorboard_dir: "runs/MinigridMemory_experiment"
ckpt_dir: "MinigridMemory/MinigridMemory_src/checkpoints"
