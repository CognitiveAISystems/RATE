{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "from lstm_agent_cql_bc import DecisionLSTM\n",
    "import yaml\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from TMaze_new.TMaze_new_src.utils.tmaze import TMazeClassicPassive\n",
    "from TMaze_new.TMaze_new_src.utils import seeds_list\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(seed, exp_name, loss_mode, stacked_input, context_length, segments):\n",
    "    agent = DecisionLSTM(4, 1, 32, num_layers=2, mode='tmaze')\n",
    "    \n",
    "    run_name = f'{exp_name}_{loss_mode}_{seed}_stacked_{stacked_input}_context_{context_length}_segments_{segments}'\n",
    "    print(run_name)\n",
    "    model_path = f'../ckpt/tmaze_ckpt_v2/{loss_mode}/{seed}/{run_name}.ckpt'\n",
    "    \n",
    "    agent.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    agent.eval()\n",
    "    agent.to(agent.device)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'tmaze_v2'\n",
    "stacked_input = False\n",
    "loss_mode = 'bc'\n",
    "segments = 3\n",
    "context_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmaze_v2_bc_1_stacked_False_context_3_segments_3\n"
     ]
    }
   ],
   "source": [
    "agent = load_model(\n",
    "    seed=1,\n",
    "    exp_name=exp_name,\n",
    "    stacked_input=stacked_input,\n",
    "    loss_mode=loss_mode, \n",
    "    context_length=context_length,\n",
    "    segments=segments\n",
    ")\n",
    "\n",
    "_ = agent.eval()\n",
    "_ = agent.to(agent.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(seed, episode_timeout, corridor_length, stacked_input, loss_mode):\n",
    "    channels = 5\n",
    "    create_video = False\n",
    "\n",
    "    env = TMazeClassicPassive(\n",
    "        episode_length=episode_timeout, \n",
    "        corridor_length=corridor_length, \n",
    "        penalty=0, \n",
    "        seed=seed, \n",
    "        goal_reward=1.0)\n",
    "\n",
    "    state = env.reset() # {x, y, hint}\n",
    "    np.random.seed(seed)\n",
    "    where_i = state[0]\n",
    "    mem_state = state[2]\n",
    "    mem_state2 = state\n",
    "\n",
    "    state = np.concatenate((state, np.array([0]))) # {x, y, hint, flag}\n",
    "    state = np.concatenate((state, np.array([np.random.randint(low=-1, high=1+1)]))) # {x, y, hint, flag, noise}\n",
    "\n",
    "    if create_video == True:\n",
    "        print(\"down, required act: 3\" if mem_state == -1.0 else \"up,  required act: 1\")\n",
    "\n",
    "    state = torch.tensor(state).reshape(1, 1, channels)\n",
    "    out_states = []\n",
    "    out_states.append(state.cpu().numpy())\n",
    "    done = True\n",
    "    Flag = 0\n",
    "    rtg = 1.0\n",
    "    agent.init_hidden(1)\n",
    "    \n",
    "    episode_return, episode_length = 0, 0\n",
    "\n",
    "    for t in range(episode_timeout):\n",
    "        with torch.no_grad():\n",
    "            q_values = []\n",
    "            for possible_action in [0, 1, 2, 3]:  # 4 –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è\n",
    "                action_tensor = torch.tensor([[[possible_action]]], \n",
    "                                        dtype=torch.float32, \n",
    "                                        device=agent.device).long()\n",
    "                rtg_tensor = torch.tensor([[[rtg]]], \n",
    "                                        dtype=torch.float32, \n",
    "                                        device=agent.device)#.long()\n",
    "                if loss_mode == 'cql':\n",
    "                    update_lstm_hidden = possible_action==3\n",
    "                else:\n",
    "                    update_lstm_hidden = True\n",
    "                    \n",
    "                action_preds, q1, q2, _ = agent.forward(\n",
    "                    states = state[:, :, 1:].cuda().float(),\n",
    "                    actions = action_tensor.cuda(),\n",
    "                    returns_to_go = rtg_tensor.cuda(),\n",
    "                    update_hidden = update_lstm_hidden,\n",
    "                    stacked_input = stacked_input,\n",
    "                )\n",
    "\n",
    "                q_value = torch.minimum(q1, q2)\n",
    "                q_values.append(q_value)\n",
    "\n",
    "                if loss_mode == 'bc':\n",
    "                    break\n",
    "\n",
    "            # Select action with max Q-value\n",
    "            if loss_mode == 'cql':\n",
    "                q_values = torch.cat(q_values, dim=-1)\n",
    "                action = torch.argmax(q_values).item() #+ 3\n",
    "            else:\n",
    "                action = torch.argmax(torch.softmax(action_preds, dim=-1).squeeze()).item()\n",
    "\n",
    "        # print(t, action, torch.softmax(action_preds, dim=-1).squeeze().detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        rtg -= reward\n",
    "        \n",
    "        if t < 0:\n",
    "            state[2] = mem_state2[2]\n",
    "        \n",
    "            # {x, y, hint} -> {x, y, hint, flag}\n",
    "        if state[0] != env.corridor_length:\n",
    "            state = np.concatenate((state, np.array([0])))\n",
    "        else:\n",
    "            if Flag != 1:\n",
    "                state = np.concatenate((state, np.array([1])))\n",
    "                Flag = 1\n",
    "            else:\n",
    "                state = np.concatenate((state, np.array([0])))\n",
    "\n",
    "        state = np.concatenate((state, np.array([np.random.randint(low=-1, high=1+1)])))\n",
    "        state = state.reshape(1, 1, channels)\n",
    "        state = torch.from_numpy(state).float().cuda()\n",
    "                \n",
    "            \n",
    "        if done:\n",
    "            if create_video == True:\n",
    "                if np.round(where_i, 4) == np.round(corridor_length, 4):\n",
    "                    print(\"Junction achieved üòÄ ‚úÖ‚úÖ‚úÖ\")\n",
    "                    print(\"Chosen act:\", \"up\" if action == 1 else \"down\" if action == 3 else \"wrong\")\n",
    "                    if mem_state == -1 and action == 3:\n",
    "                        print(\"Correct choice üòÄ ‚úÖ‚úÖ‚úÖ\")\n",
    "                    elif mem_state == 1 and action == 1:\n",
    "                        print(\"Correct choice üòÄ ‚úÖ‚úÖ‚úÖ\")\n",
    "                    else:\n",
    "                        print(\"Wrong choice üò≠ ‚õîÔ∏è‚õîÔ∏è‚õîÔ∏è\")\n",
    "                else:\n",
    "                    print(\"Junction is not achieved üò≠ ‚õîÔ∏è‚õîÔ∏è‚õîÔ∏è\")\n",
    "            break \n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0, reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1, reward: 1.0\n",
      "seed: 2, reward: 1.0\n",
      "seed: 5, reward: 1.0\n",
      "1.0 0.0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:01,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0, reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 1, reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:01<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 2, reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 5, reward: 1.0\n",
      "1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "episode_timeout = 9 # segments * context_length\n",
    "corridor_length = episode_timeout - 2\n",
    "\n",
    "rewards = []\n",
    "for seed in tqdm([0,1,2,5]): # seeds_list[::25]\n",
    "    reward = run_episode(seed=seed, episode_timeout=episode_timeout, corridor_length=corridor_length, stacked_input=stacked_input, loss_mode=loss_mode)\n",
    "    rewards.append(reward)\n",
    "    print(f\"seed: {seed}, reward: {reward}\")\n",
    "\n",
    "print(np.mean(rewards), np.std(rewards))\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "episode_timeout = 90 # segments * context_length\n",
    "corridor_length = episode_timeout - 2\n",
    "\n",
    "rewards = []\n",
    "for seed in tqdm([0,1,2,5]): # seeds_list[::25]\n",
    "    reward = run_episode(seed=seed, episode_timeout=episode_timeout, corridor_length=corridor_length, stacked_input=stacked_input, loss_mode=loss_mode)\n",
    "    rewards.append(reward)\n",
    "    print(f\"seed: {seed}, reward: {reward}\")\n",
    "\n",
    "print(np.mean(rewards), np.std(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
